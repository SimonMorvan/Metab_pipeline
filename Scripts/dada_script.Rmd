---
title: "DADA2 pipeline"
author: "Alexis Carteron & Simon Morvan"
date: "9 mars 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# DADA2 pipeline
# Designed for sequences amplified with ITS marker gene and generated by Illumina 2x300 bp paired-end


#### NB: Samples must be demultiplexed (split into individual per-sample fastqs)

## Load packages and data
library(dada2)

## Paths and lists ##
path <- "data/ITS_sub/"
list.files(path)

#### Read in the names of the fastq files, and perform some string manipulation to get lists of the forward and reverse fastq files in matched order
# Sort ensures forward/reverse reads are in same order
fnFs <- sort(list.files(path, pattern="_R1.fastq"))
fnRs <- sort(list.files(path, pattern="_R2.fastq"))

# Extract sample names, assuming filenames have format: SAMPLENAME_XXX.fastq
sample.names <- sapply(strsplit(fnFs, "_"), `[`, 1)
sample.names 

# Specify the full path to the fnFs and fnRs
fnFs <- file.path(path, fnFs)
fnRs <- file.path(path, fnRs)

## Examine quality profiles ##
## This is to help with filtering and trimming choices
##  Visualize the quality profiles by sample
plotQualityProfile(fnFs[1:4]) # forward reads
plotQualityProfile(fnRs[1:4]) # reverse reads

# All samples aggregated
# plotQualityProfile(fnFs) # forward reads


# NB: In the figures the mean is in green, the median the solid orange line and the quartiles are the dotted orange lines.
# Phred score of 10 => 90 % precision, 20 => 99 % precision, 30 => 99.9 % precision

#### Perform filtering and trimming ####
filt_path <- file.path(path, "filtered_pairedend") # Place filtered files in filtered/ subdirectory
filtFs <- file.path(filt_path, paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(filt_path, paste0(sample.names, "_R_filt.fastq.gz"))

# Filter the forward and reverse reads:
out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, 
                     maxN=0,
                     truncLen = c(280,280),
                     maxEE=c(2,2), 
                     truncQ=6,
                     rm.phix=TRUE,
                     trimLeft=c(18,20),
                     compress=TRUE, 
                     multithread=TRUE)

# MaxEE sets the maximum number of “expected errors” allowed in a read, which is a better filter than simply averaging quality scores.
# It can undesirable to truncate reads to a fixed length due to the large amount of length variation at that locus.
All amplicons were > 300 bp
# trimLeft() in function of the primers that are used:
# ITS3_KYO2	GATGAAGAACGYAGYRAA = 18bp (forward)
# ITS4	TCCTCCGCTTATTGATATGC = 20bp (reverse)

# Check pourcentage of discarded reads
#pourc <- cbind(out[,2]/out[,1])
#plot(out)
#plot(pourc)
#pourc_disc <- cbind(out, pourc)
#pourc_disc 
mean(out[,2])/mean(out[,1])

#### Learn the Error Rates ####
# It learns the error model from the data, by alternating estimation of the error rates and inference of sample
errF <- learnErrors(filtFs, nreads = 1e+07, # using all the reads. 
                    multithread=TRUE)
#If nreads = 1e+06, convergence is after 5 rounds and Total reads used is 1013240. PlotErros output very very similar.

errR <- learnErrors(filtRs, nreads = 1e+07, 
                    multithread=TRUE)

# Visualize the estimated error rates
plotErrors(errF, nominalQ=TRUE)
plotErrors(errR, nominalQ=TRUE)

# PS
# The error rates for each possible transition (eg. A->C, A->G, …) are shown. 
# Points are the observed error rates for each consensus quality score. 
# The black line shows the estimated error rates after convergence. 
# The red line shows the error rates expected under the nominal definition of the Q-value (for Illumina technology). 

#### Dereplication ####

#combines all identical sequencing reads into into “unique sequences” with a corresponding “abundance”
#reduces computation time by eliminating redundant comparisons
derepFs <- derepFastq(filtFs, 
                      n = 1e+06) # no change obseved when n = 1e+07

derepRs <- derepFastq(filtRs, 
                      n = 1e+06)
# The consensus quality profile of a unique sequence is the average of the positional qualities from the dereplicated reads.

# Name the derep-class objects by the sample names
names(derepFs) <- sample.names
names(derepRs) <- sample.names

#### Sample Inference ####

# Infer the sequence variants in each sample
dadaFs <- dada(derepFs, 
               err=errF, 
               BAND_SIZE = 16, # Default is 16, based on 16S amplicon data that has lower indel rates than ITS. Only cost: run-time is longer. No differences observed when = 32
               KDIST_CUTOFF = 0.42, # Default is 0.42, based on 16S amplicon data. No differences observed when = .6
               pool=TRUE,
               multithread=TRUE)
dadaFs[[1]]

dadaRs <- dada(derepRs, 
               err=errR,
               pool=TRUE,
               multithread=TRUE)
dadaRs[[1]]

#save(dadaFs, file="dada2/saved_table/dadaFs.rdata")
#save(dadaRs, file="dada2/saved_table/dadaRs.rdata")

#### Merging ####
mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs, 
                      minOverlap = 12, 
                      maxMismatch = 0, 
                      returnRejects = FALSE, 
                      propagateCol = character(0),
                      justConcatenate = FALSE, 
                      trimOverhang = FALSE)
# That way paired reads that did not exactly overlap were removed

# if returnRejects = TRUE
# pairs that that were rejected based on mismatches in the overlap region are retained BUT with accept = FALSE in table

# Inspect the merger data.frame from the first sample
# head(mergers[[1]])
# tail(mergers[[3]])
# max(mergers[[1]]$nmatch)
# min(mergers[[1]]$nmatch)

#### Construct sequence table ####

# SV version of the OTU table
seqtab <- makeSequenceTable(mergers)
dim(seqtab)

# Inspect distribution of sequence lengths
table(nchar(getSequences(seqtab)))

#### Remove chimeras ####
seqtab.nochim <- removeBimeraDenovo(seqtab, 
                                    method="pooled", # The samples in the sequence table are all pooled together for bimera identification.
                                    multithread=TRUE, verbose=TRUE) 
# Identified 11 bimeras out of 521 input sequences

dim(seqtab.nochim)
sum(seqtab.nochim)/sum(seqtab) #  percentage of the total sequence reads

# Inspect distribution of sequence lengths
table(nchar(getSequences(seqtab.nochim)))

# save sequence table
# save(seqtab.nochim, file="data/seqtab.nochim.rdata")

# Exclude singletons and doubletons
seqtab.nochim.nosd = seqtab.nochim[,colSums(seqtab.nochim) > 200]

#### Assign taxonomy ####
# https://unite.ut.ee/repository.php
taxo <- assignTaxonomy(seqtab.nochim.nosd, "reference_database/sh_general_release_dynamic_01.12.2017.fasta", 
                              minBoot = 50, #Default 50. The minimum bootstrap confidence for assigning a taxonomic level.
                              multithread=FALSE, verbose=TRUE)

unique(unname(taxo[,7]))
head(taxo)
# save it
# save(taxo, file="data/taxo.30.04.rdata")

# NB1: dada2 does not throw away singleton reads. 
# However, it does not infer biological sequence variants that are only supported by a single read - singletons are assumed too difficult to differentiate from errors. 
# Hence no singletons in the output table of amplicon sequence variants.

# NB2: DADA2 consistently measures diversity across different filtering parameters and error rates. OTU methods do not.
# https://twitter.com/bejcal/status/771010634074820608

# NB3: The ASVs with no species assignment do not match the same species in over 50% of the bootstrap replicate kmer-based assignments 
# (see Wang et al. 2007 for more info on the naive Bayesian classifier method).


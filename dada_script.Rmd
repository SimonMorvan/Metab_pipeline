---
title: "DADA2 pipeline"
author: "Alexis Carteron & Simon Morvan"
date: "9 mars 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.


## DADA2 pipeline
# Designed for sequences amplified with ITS marker gene and generated by Illumina 2x300 bp paired-end

## Ressources
# Callahan, B. J., McMurdie, P. J., Rosen, M. J., Han, A. W., Johnson, A. J. A., & Holmes, S. P. (2016). DADA2: high-resolution sample inference from Illumina amplicon data. Nature methods, 13(7), 581-583.
# http://benjjneb.github.io/dada2/tutorial.html

# NB: Samples must be demultiplexed (split into individual per-sample fastqs)

#### Load packages and data ####
library(dada2); packageVersion("dada2")
# ‘1.5.8’

# rename file in the terminal
# for f in *; do mv "$f" "${f:12}"; done

#### Paths ####
path <- "seq"
list.files(path)

# Read in the names of the fastq files, and perform some string manipulation to get lists of the forward and reverse fastq files in matched order:
# Sort ensures forward/reverse reads are in same order
fnFs <- sort(list.files(path, pattern="_R1.fastq"))
fnRs <- sort(list.files(path, pattern="_R2.fastq"))

# Extract sample names, assuming filenames have format: SAMPLENAME_XXX.fastq
sample.names <- sapply(strsplit(fnFs, "_"), `[`, 1)
sample.names 

# Specify the full path to the fnFs and fnRs
fnFs <- file.path(path, fnFs)
fnRs <- file.path(path, fnRs)

#### Examine quality profiles of forward and reverse reads ####
##  Visualize the quality profiles by sample or all combined
# Forward Reads
plotQualityProfile(fnFs[1:4], n = 5e+05) # forward reads
plotQualityProfile(fnFs, n = 5e+06, aggregate = TRUE) 

# Reverse Reads
plotQualityProfile(fnRs[1:4], n = 5e+05) # reverse reads
plotQualityProfile(fnRs, n = 5e+06, aggregate = TRUE) 

# NB In the figures: the mean is in green, the median the solid orange line and the quartiles are the dotted orange lines.
# Phred score of 10 => 90 % precision, 20 => 99 % precision, 30 => 99.9 % precision

#### Perform filtering and trimming ####

filt_path <- file.path(path, "filtered_pairedend") # Place filtered files in filtered/ subdirectory
filtFs <- file.path(filt_path, paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(filt_path, paste0(sample.names, "_R_filt.fastq.gz"))

# Filter the forward and reverse reads:
out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, 
                     maxN=0,
                     truncLen = c(290,290),
                     maxEE=c(3,3), 
                     truncQ=6,
                     rm.phix=TRUE,
                     trimLeft=c(18,20),
                     compress=TRUE, 
                     multithread=TRUE)

# PS
# Standard filtering parameters : maxN=0 (DADA2 requires no Ns), truncQ=2, rm.phix=TRUE, maxEE=2

# MaxEE sets the maximum number of “expected errors” allowed in a read, which is a better filter than simply averaging quality scores.
# /!\ For common ITS amplicon strategies, it is undesirable to truncate reads to a fixed length due to the large amount of length variation at that locus.

# no difference up to truncQ=6 (probably mean it is already been filtered by Genome Quebec). When truncQ=7 only 0.2707456 % kept.

# Primers used are:
# ITS3_KYO2	GATGAAGAACGYAGYRAA = 18bp (forward)
# ITS4	TCCTCCGCTTATTGATATGC = 20bp (reverse)
# Therefore trimLeft=c(18,20)


# Check pourcentage of discarded reads
#pourc <- cbind(out[,2]/out[,1])
#plot(out)
#plot(pourc)
#pourc_disc <- cbind(out, pourc)
#pourc_disc 
mean(out[,2])/mean(out[,1])
# overall mean of discarded reads
# 0.1778212 20/02/2018 with maxEE(1,1), truncLen = 0
# 0.2511611 with maxEE(1,1), truncLen = c(290,290)
# 0.5670382 with maxEE(3,3), truncLen = c(290,290)
# 0.4878112 with trunclen=0 and maxEE=c(3,3)
# 0.353185 with trunclen=0 and maxEE=c(2,2)
# 0.5867783 with maxEE(3,3), truncLen = c(290,285)

#### Learn the Error Rates ####
# It learns the error model from the data, by alternating estimation of the error rates and inference of sample
errF <- learnErrors(filtFs, nreads = 1e+07, # using all the reads. 
                    multithread=TRUE)
#If nreads = 1e+06, convergence is after 5 rounds and Total reads used is 1013240. PlotErros output very very similar.

errR <- learnErrors(filtRs, nreads = 1e+07, 
                    multithread=TRUE)

# Visualize the estimated error rates
plotErrors(errF, nominalQ=TRUE)
#ggsave(filename = "plot.errorF", path = "dada2/figures/", device = "pdf")
plotErrors(errR, nominalQ=TRUE)
#ggsave(filename = "plot.errorR", path = "dada2/figures/", device = "pdf")

# PS
# The error rates for each possible transition (eg. A->C, A->G, …) are shown. 
# Points are the observed error rates for each consensus quality score. 
# The black line shows the estimated error rates after convergence. 
# The red line shows the error rates expected under the nominal definition of the Q-value (for Illumina technology). 

#### Dereplication ####

#combines all identical sequencing reads into into “unique sequences” with a corresponding “abundance”
#reduces computation time by eliminating redundant comparisons
derepFs <- derepFastq(filtFs, 
                      n = 1e+06) # no change obseved when n = 1e+07

derepRs <- derepFastq(filtRs, 
                      n = 1e+06)
# The consensus quality profile of a unique sequence is the average of the positional qualities from the dereplicated reads.

# Name the derep-class objects by the sample names
names(derepFs) <- sample.names
names(derepRs) <- sample.names

#### Sample Inference ####

# Infer the sequence variants in each sample
dadaFs <- dada(derepFs, 
               err=errF, 
               BAND_SIZE = 16, # Default is 16, based on 16S amplicon data that has lower indel rates than ITS. Only cost: run-time is longer. No differences observed when = 32
               KDIST_CUTOFF = 0.42, # Default is 0.42, based on 16S amplicon data. No differences observed when = .6
               pool=TRUE,
               multithread=TRUE)
# 81 samples were pooled: 2897051 reads in 1215639 unique sequences. 02/02
# 81 samples were pooled: 3484804 reads in 1355886 unique sequences. 20/02

dadaRs <- dada(derepRs, 
               err=errR,
               pool=TRUE,
               multithread=TRUE)
# 81 samples were pooled: 2897051 reads in 1058742 unique sequences. 02/02
# 81 samples were pooled: 3484804 reads in 1110007 unique sequences. 20/02
dadaFs[[1]]
# 168 sample sequences were inferred from 8815 input unique sequences.

dadaRs[[1]]
# 155 sample sequences were inferred from 10098 input unique sequences.

#save(dadaFs, file="dada2/saved_table/dadaFs.rdata")
#save(dadaRs, file="dada2/saved_table/dadaRs.rdata")
#save(derepFs, file="dada2/saved_table/derepFs.rdata")
#save(derepRs, file="dada2/saved_table/derepRs.rdata")

#### Merging ####

mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs, 
                      minOverlap = 12, 
                      maxMismatch = 0, 
                      returnRejects = FALSE, 
                      propagateCol = character(0),
                      justConcatenate = FALSE, 
                      trimOverhang = FALSE)
# That way paired reads that did not exactly overlap were removed

# if returnRejects = TRUE
# pairs that that were rejected based on mismatches in the overlap region are retained BUT with accept = FALSE in table

# Inspect the merger data.frame from the first sample
# head(mergers[[1]])
# tail(mergers[[3]])
# max(mergers[[1]]$nmatch)
# min(mergers[[1]]$nmatch)

#### Construct sequence table ####

# SV version of the OTU table
seqtab <- makeSequenceTable(mergers)
dim(seqtab)
# 81 2704. 07/02/2018
# 81 3845. 17/10/2017
# 81 2124. 20/02/2018 with maxEE(1,1), truncLen = 0 or truncLen = 10

# Inspect distribution of sequence lengths
table(nchar(getSequences(seqtab)))

# Inspect distribution of sequence lengths
# huge variance in length, from 282 to 535 (idem with maxEE(1,1), truncLen = 0)
# write.table(length.var, file="dada2/saved_table/seqtab.length.var.ITS.paired.txt", row.names=TRUE, col.names=TRUE)
# pdf("dada2/saved_table/seqtab.length.var.plot.pdf")
# seqtab.length.var.plot <- plot(table(nchar(getSequences(seqtab))))
# dev.off()

#### Remove chimeras ####
seqtab.nochim <- removeBimeraDenovo(seqtab, 
                                    method="pooled", # The samples in the sequence table are all pooled together for bimera identification. Default = consensus
                                    multithread=TRUE, verbose=TRUE) 
# Identified 623 bimeras out of 3845 input sequences
# method="pooled" is longer but identified 623 bimeras vs 502 for method="consensus" (!!!)

dim(seqtab.nochim)
# 81 2222
sum(seqtab.nochim)/sum(seqtab) #  percentage of the total sequence reads
# 0.9602591

# Inspect distribution of sequence lengths
table(nchar(getSequences(seqtab.nochim)))

# save seq table
# write.table(seqtab.nochim, file="dada2/saved_table/seqtab.nochim.ITS.paired.txt", row.names=TRUE, col.names=TRUE)
# save(seqtab.nochim, file="dada2/saved_table/seqtab.nochim.ITS.paired.21.02.rdata")


#### Assign taxonomy ####

taxa.paired <- assignTaxonomy(seqtab.nochim, "reference_database/sh_general_release_dynamic_s_01.12.2017.RefS.fasta", 
                              minBoot = 50, #Default 50. The minimum bootstrap confidence for assigning a taxonomic level.
                              multithread=TRUE, verbose=TRUE)
nrow(unname(taxa.paired)) #2688 for minboot = 50 21/02
length(unique(unname(taxa.paired[,7]))) # 305/470 species identified (minboot80/50)
length(unique(unname(taxa.paired.RefS[,7]))) # 306 species identified

unique(unname(taxa.paired[,6])) # 353/400 genus identified (256 with maxEE(1,1), truncLen = 0)
unique(unname(taxa.paired[,5])) # 207/222 family identified (170 with maxEE(1,1), truncLen = 0)
unique(unname(taxa.paired[,4])) # 93/95 order identified
unique(unname(taxa.paired[,3])) # 40/42 class identified
unique(unname(taxa.paired[,2])) # 15/16 Phylum identified

# save it
# write.csv(taxa.paired, file = "dada2/saved_table/assigntaxaITS.paired.RefS.21.02.csv")
# save(taxa.paired, file="dada2/saved_table/taxa.ITS.paired.21.02.minboot50.rdata")


###test
taxa.paired <- assignTaxonomy(seqtab.nochim, "/home/udem/Downloads/UNITE_public_01.12.2017.fasta", 
                              minBoot = 50, #Default 50. The minimum bootstrap confidence for assigning a taxonomic level.
                              multithread=TRUE, verbose=TRUE)

# Format to use Funguild database ####
taxa.guild <- cbind(rownames(taxa.paired), paste(taxa.paired[,1], taxa.paired[,2], taxa.paired[,3], taxa.paired[,4], taxa.paired[,5], taxa.paired[,6], taxa.paired[,7]))
colnames(taxa.guild) <- c("ESV_ID", "taxonomy")
# write.table(taxa.guild, file = "guild/taxa.funguild.21.02.minboot50.txt", row.names = FALSE, col.names = TRUE, quote = FALSE, sep = "\t")


# NB1: dada2 does not throw away singleton reads. 
# However, it does not infer biological sequence variants that are only supported by a single read - singletons are assumed too difficult to differentiate from errors. 
# Hence no singletons in the output table of amplicon sequence variants.

# NB2: DADA2 consistently measures diversity across different filtering parameters and error rates. OTU methods do not.
# https://twitter.com/bejcal/status/771010634074820608

# NB3: The ASVs with no species assignment do not match the same species in over 50% of the bootstrap replicate kmer-based assignments 
# (see Wang et al. 2007 for more info on the naive Bayesian classifier method).

